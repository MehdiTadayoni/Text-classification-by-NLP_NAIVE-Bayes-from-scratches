{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "## A",
    "### Mehdi Tadayoni     \n",
    "###   \n",
    "### ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## 1. introduction\n",
    "The main target is to implement a Naive Bayes algorithm from scratch that is able to predict the domain - one of Archaea, Bacteria, Eukaryota or Virus - from the abstract of research papers about proteins taken from the MEDLINE database.\n",
    "this task is divided different part such as preprocessing, Naive Bayes class (includes different functions), import the data, splitting data to test and trainset, evaluation of test set data and export the new test_set data to submit to the kaggle. \n",
    "\n",
    "## 2. preprocessing\n",
    "in this step str_arg and cleaned_str are the input and output of preprocessing step, respectively. the function will do the below task:\n",
    "1. everything apart from letters in all words is excluded\n",
    "2. multiple spaces are replaced by single space\n",
    "3. input is converted to lower_case \n",
    "\n",
    "## 3. NaiveBayes Classifier\n",
    "It is a generic code for the NaiveBayes Classifier and it will be defining a NaiveBayes class and includes relevant functions inside this class. However, it includes diverse function as below:\n",
    "### 3-1 def addToBow(self,example,id_dic):\n",
    "this function includes 'example' as an input and 'id_dic' which implies to which Bag of Words category this example belongs to\n",
    "totally this function implement splits the input (example) on the basis of spaces as a tokenizer and adds every tokenized vocabulary to\n",
    "its corresponding dictionary or Bag of Words\n",
    "### 3-2 def train(self,dataset,labels):\n",
    "This function will train will train the Naive Bayes Model. the important task is computing a Bag of Words for each class or labels. re and collections are python packages which are used for NB training. in this assignment, pandas only used once for constructing the cleaned_examples (pd.DataFrame()).\n",
    "in this step we have to calculate ([ count(w|c)+1 ] / [ count(c) + |V| + 1 ] } * p(c)) as below:\n",
    "1. prior_ probability of each label ( A,B,E,V) as p(c)\n",
    "2. vocabulary |V|\n",
    "3. denominator value for all classes - [ count(c) + |V| + 1 ] \n",
    "We can do all these steps at test time, but it can be time consuming so we will precompute all of them and apply them during test time to speed up predictions.\n",
    "### 3-3 def getExampleProb(self,test_example):                                \n",
    "Likelihood (probLike) and Posterior (probpost) probability are main outputs in this step\n",
    "### 3-4 def test(self,test_set)\n",
    "The main objective in this step is determining probability of each test data for all classes and predicts the correct class against which the label probability is the best\n",
    "## 4-Splitting the data  \n",
    "the data was splitted to train and test set data (80% and 20%).\n",
    "## 5-Evaluation of test result Using the NB Model:\n",
    "the accuracy for test set data is about 95%\n",
    "##  6- Evaluation new data by kaggle:\n",
    "the test set data (tst.csv) was evaluated and after uploading to kaggle the accuracy is about 0.96 which was submitted in 10/4/2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets first write a handy text preprocessing function which is not part of the NaiveBayes class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(str_arg):\n",
    "    cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) \n",
    "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) \n",
    "    cleaned_str=cleaned_str.lower()  \n",
    "    return cleaned_str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,label_cl):\n",
    "        # making unique number of labels\n",
    "        self.classes=label_cl \n",
    "        \n",
    "    def addToBow(self,example,id_dic):\n",
    "        if isinstance(example,np.ndarray): example=example[0]\n",
    "        for token_vocab in example.split(): #for all vacabolary in preprocessed example\n",
    "                      self.bow_dicts[id_dic][token_vocab]+=1  \n",
    "                \n",
    "#------training function which can train the Naive Bayes Model-------------------------------------------------                \n",
    "    def train(self,dataset,labels):\n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        #converting to numpy arrays\n",
    "        if not isinstance(self.examples,np.ndarray):\n",
    "            self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.labels,np.ndarray):\n",
    "            self.labels=np.array(self.labels)\n",
    "            \n",
    "        #constructing bag of words \n",
    "        for ct_index,ct in enumerate(self.classes):\n",
    "            all_ct_examples=self.examples[self.labels==ct] \n",
    "            cleaned_examples=[preprocess_string(ct_example) for ct_example in all_ct_examples]\n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,ct_index)\n",
    "            \n",
    " #---------------------------- Probabilities calculations ------------------------------------------------------------              \n",
    "      \n",
    "        labelPro=np.empty(self.classes.shape[0])\n",
    "        vocabs=[]\n",
    "        ct_vocab_counts=np.empty(self.classes.shape[0])\n",
    "        for ct_index,ct in enumerate(self.classes):\n",
    "           \n",
    "            # prior probability etimation\n",
    "            labelPro[ct_index]=np.sum(self.labels==ct)/float(self.labels.shape[0]) \n",
    "            \n",
    "            # all counts of vocab estimation\n",
    "            count=list(self.bow_dicts[ct_index].values())\n",
    "            ct_vocab_counts[ct_index]=np.sum(np.array(list(self.bow_dicts[ct_index].values())))+1 \n",
    "            \n",
    "            #get all words of this category                                \n",
    "            vocabs+=self.bow_dicts[ct_index].keys()\n",
    "                                                     \n",
    "        #combine all vocab\n",
    "        \n",
    "        self.vocab=np.unique(np.array(vocabs))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                                                    \n",
    "        denoms=np.array([ct_vocab_counts[ct_index]+self.vocab_length+1 for ct_index,ct in enumerate(self.classes)])                                                                          \n",
    "\n",
    "        #self.ct_info has a tuple of values such as dict at index 0, prior probability at index 1, denominator value at index 2\n",
    "        self.ct_info=[(self.bow_dicts[ct_index],labelPro[ct_index],denoms[ct_index]) for ct_index,cat in enumerate(self.classes)]                               \n",
    "        self.ct_info=np.array(self.ct_info)                                 \n",
    "                                              \n",
    "  #---------------------------------------------Estimates posterior probability --------------------------------------------------------                                            \n",
    "    def getExampleProb(self,test_example):                                \n",
    "        \n",
    "        #Likelihood permeability estimation                         \n",
    "        probLike=np.zeros(self.classes.shape[0])\n",
    "        \n",
    "        #finding probability for each label \n",
    "        for ct_index,ct in enumerate(self.classes): \n",
    "                             \n",
    "            for token in test_example.split():                                               \n",
    "                token_counts=self.ct_info[ct_index][0].get(token,0)+1\n",
    "                \n",
    "                #getting likelihood                              \n",
    "                token_prob=token_counts/float(self.ct_info[ct_index][2])                              \n",
    "                probLike[ct_index]+=np.log(token_prob)\n",
    "                                              \n",
    "        # likelihood value for every label \n",
    "        probpost=np.empty(self.classes.shape[0])\n",
    "        for ct_index,ct in enumerate(self.classes):\n",
    "            probpost[ct_index]=probLike[ct_index]+np.log(self.ct_info[ct_index][1])                                  \n",
    "      \n",
    "        return probpost\n",
    "    \n",
    " #------------------------------------------- Estimation of probability for each labels\n",
    "    def test(self,test_set):\n",
    "           \n",
    "        prediction=[] \n",
    "        for example in test_set:                           \n",
    "            # same approach as training set                                   \n",
    "            cleaned_example=preprocess_string(example) \n",
    "            # posterior probability                                  \n",
    "            probpost=self.getExampleProb(cleaned_example) #get prob of this example for both classes\n",
    "            #selecting max value \n",
    "            prediction.append(self.classes[np.argmax(probpost)])\n",
    "        return np.array(prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.genfromtxt(\"trg.csv\", dtype=None, encoding=None, delimiter=\",\",skip_header=1,usecols=[1,2])\n",
    "X=df[:,1]\n",
    "y=df[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Training data:  3200\n",
      " Number of Training Labels:  3200\n"
     ]
    }
   ],
   "source": [
    "sp=int(0.8*len(df))\n",
    "train_data, test_data= X[:sp], X[sp:]\n",
    "train_labels, test_labels= y[:sp], y[sp:]\n",
    "print (\" Number of Training data: \",len(train_data))\n",
    "print (\" Number of Training Labels: \",len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Test Examples:  800\n",
      "Number of Test Labels:  800\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of Test Examples: \",len(test_data))\n",
    "print (\"Number of Test Labels: \",len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training by Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase starts \n",
      "Training phase finished\n"
     ]
    }
   ],
   "source": [
    "nb=NaiveBayes(np.unique(train_labels)) \n",
    "print (\"Training phase starts \")\n",
    "nb.train(train_data,train_labels)\n",
    "print ('Training phase finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of test result Using the NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set data:  800\n",
      "Test Set Accuracy:  95.0 %\n"
     ]
    }
   ],
   "source": [
    "nbclasses=nb.test(test_data) \n",
    "test_Eval=np.sum(nbclasses==test_labels)/float(test_labels.shape[0]) \n",
    "\n",
    "print (\"Test Set data: \",test_labels.shape[0])\n",
    "print (\"Test Set Accuracy: \",test_Eval*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test set dataset tst.csv\n",
    "df_test = np.array(np.genfromtxt(\"tst.csv\", dtype=None, encoding=None, delimiter=\",\",skip_header=0 ))\n",
    "Xtest=np.array(df_test[:,1])\n",
    "#generating predictions....\n",
    "classes=np.array(nb.test(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only using pandas package here for uploading csv data to Kaggle\n",
    "import pandas as pd\n",
    "test=pd.read_csv('tst.csv')\n",
    "Xtest=test.iloc[:,1]\n",
    "classes=nb.test(Xtest) \n",
    "kaggle_df=pd.DataFrame(data=np.column_stack([test[\"id\"].values,classes]),columns=[\"id\",'class'])\n",
    "kaggle_df.to_csv(\"mtad786_v1.csv\",index=False)\n",
    "print ('Predcitions Generated and saved to mtad786_v1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
